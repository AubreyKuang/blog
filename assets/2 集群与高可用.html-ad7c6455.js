import{_ as t,V as i,W as o,X as e,Y as s,$ as a,a0 as r,E as l}from"./framework-91a010c2.js";const p={},d=r('<h2 id="redis集群与高可用" tabindex="-1"><a class="header-anchor" href="#redis集群与高可用" aria-hidden="true">#</a> Redis集群与高可用</h2><h3 id="如何实现服务高可用" tabindex="-1"><a class="header-anchor" href="#如何实现服务高可用" aria-hidden="true">#</a> 如何实现服务高可用</h3><p>从Redis的多服务节点考虑</p><h4 id="主从复制" tabindex="-1"><a class="header-anchor" href="#主从复制" aria-hidden="true">#</a> <strong>主从复制</strong></h4><p>Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是**「读写分离」**的方式。</p><p>主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器</p><p>从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。</p><blockquote><p>主从服务器之间的命令复制是<strong>异步</strong>进行的。</p><p>具体来说，在主从服务器命令传播阶段，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器<strong>并不会等到从服务器实际执行</strong>完命令后，再把结果返回给客户端，而是主服务器<strong>自己在本地执行完</strong>命令后，就会向客户端返回结果了。</p><p>如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。</p><p>所以，无法保证主从数据时时刻刻一致，数据不一致是难以避免的。</p></blockquote><p><strong>实现</strong></p><p>主从服务器间的第一次同步的过程可分为三个阶段：</p><ul><li>第一阶段是建立链接、协商同步；</li><li>第二阶段是主服务器同步数据给从服务器；</li><li>第三阶段是主服务器发送新写操作命令给从服务器。</li></ul><p>完成第一次同步后，双方维护一个 TCP 连接：</p><p>后续主服务器可以通过这个连接继续将写操作命令传播给从服务器，然后从服务器执行该命令，使得与主服务器的数据库状态相同。</p><p>是长连接的，避免频繁的 TCP 连接和断开带来的性能开销。</p><h5 id="网络断开-如何保证主从服务器数据一致性" tabindex="-1"><a class="header-anchor" href="#网络断开-如何保证主从服务器数据一致性" aria-hidden="true">#</a> 网络断开，如何保证主从服务器数据一致性？</h5><p>在 Redis 2.8 之前，如果主从服务器在命令同步时出现了网络断开又恢复的情况，从服务器就会和主服务器<strong>重新进行一次全量复制</strong>。</p><p>所以，从 Redis 2.8 开始，网络断开又恢复后，从主从服务器会采用<strong>增量复制</strong>的方式继续同步，也就是只会把<strong>网络断开期间</strong>主服务器接收到的写操作命令，同步给从服务器。</p><ul><li>从服务器在恢复网络后，会发送 psync 命令给主服务器，此时的 psync 命令里的 offset 参数不是 -1；</li><li>主服务器收到该命令后，然后用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据；</li><li>然后主服务将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令。</li></ul><p>怎么知道发送哪些增量数据？</p><ul><li><strong>repl_backlog_buffer</strong>，是一个「<strong>环形</strong>」缓冲区，用于主从服务器断连后，从中找到差异的数据；</li><li><strong>replication offset</strong>，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己「<em>写</em>」到的位置，从服务器使用 slave_repl_offset 来记录自己「<em>读</em>」到的位置。</li></ul><h4 id="哨兵模式" tabindex="-1"><a class="header-anchor" href="#哨兵模式" aria-hidden="true">#</a> <strong>哨兵模式</strong></h4><p>使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。</p><p>为了解决这个问题，Redis 增加了哨兵模式（<strong>Redis Sentinel</strong>）</p><p>因为哨兵模式做到了可以监控主从服务器，并且提供<strong>主从节点故障转移的功能。</strong></p><h4 id="切片集群" tabindex="-1"><a class="header-anchor" href="#切片集群" aria-hidden="true">#</a> <strong>切片集群</strong></h4><p>当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 <strong>Redis 切片集群</strong>（Redis Cluster ）方案</p><p><strong>将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。</strong></p><p>采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，<strong>一个切片集群共有 16384 个哈希槽</strong>，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步：</p>',28),g={href:"https://en.wikipedia.org/wiki/Cyclic_redundancy_check",target:"_blank",rel:"noopener noreferrer"},h=e("li",null,"再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。",-1),c=r('<p>接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案：</p><ul><li><strong>平均分配：</strong> 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。</li><li><strong>手动分配：</strong> 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。</li></ul><h4 id="脑裂" tabindex="-1"><a class="header-anchor" href="#脑裂" aria-hidden="true">#</a> 脑裂</h4><p>在 Redis 主从架构中，部署方式一般是「一主多从」，主节点提供写操作，从节点提供读操作。 如果主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的<strong>网络是正常</strong>的，这个客户端并不知道 Redis 内部已经出现了问题，还在照样的向这个失联的主节点写数据（过程A），此时这些数据被旧主节点缓存到了缓冲区里，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。</p><p>这时，哨兵也发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出问题了），于是哨兵就会在「从节点」中选举出一个 leader 作为主节点，这时集群就<strong>有两个主节点了</strong> —— <strong>脑裂出现了</strong>。</p><p>然后，网络突然好了，哨兵因为之前已经选举出一个新主节点了，它就会把旧主节点降级为从节点（A），然后从节点（A）会向新主节点请求数据同步，<strong>因为第一次同步是全量同步的方式，此时的从节点（A）会清空掉自己本地的数据，然后再做全量同步。所以，之前客户端在过程 A 写入的数据就会丢失了，也就是集群产生脑裂数据丢失的问题</strong>。</p><p><strong>总结：</strong></p><p>由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。</p><p><strong>解决方案</strong>：</p><p>当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。</p><p>Redis配置文件的两个参数：</p><ul><li>min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。</li><li>min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。</li></ul><p>即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，<strong>原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了</strong>。</p><p><strong>等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。</strong></p><h3 id="判断节点是否工作" tabindex="-1"><a class="header-anchor" href="#判断节点是否工作" aria-hidden="true">#</a> 判断节点是否工作</h3><p>通过互相的 ping-pong 心态检测机制，如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。</p><ul><li>Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率。</li><li>Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量，目的是为了： <ul><li>实时监测主从节点网络状态；</li><li>上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据。</li></ul></li></ul><h3 id="" tabindex="-1"><a class="header-anchor" href="#" aria-hidden="true">#</a></h3>',18);function u(_,f){const n=l("ExternalLinkIcon");return i(),o("div",null,[d,e("ul",null,[e("li",null,[s("根据键值对的 key，按照 "),e("a",g,[s("CRC16 算法 (opens new window)"),a(n)]),s("计算一个 16 bit 的值。")]),h]),c])}const x=t(p,[["render",u],["__file","2 集群与高可用.html.vue"]]);export{x as default};
